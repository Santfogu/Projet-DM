{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3ee888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358f0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text acronym  \\\n",
      "0  LRA  limite de résistance des attelages PAR po...     PAR   \n",
      "1  LRA  limite de résistance des attelages PAR po...     PAR   \n",
      "2  LRA  limite de résistance des attelages PAR po...     PAR   \n",
      "3  LRA  limite de résistance des attelages PAR po...     PAR   \n",
      "4                               Désigna -tion des PN      PN   \n",
      "\n",
      "                                         option_text  label  \n",
      "0                           Plan d'action régularité      0  \n",
      "1  Poste d'aiguillage et de régulation : assure l...      1  \n",
      "2                                    PONT DE L'ARCHE      0  \n",
      "3                             Plan d'action régional      0  \n",
      "4  Passages à niveau : fichier des pn, recensemen...      0  \n"
     ]
    }
   ],
   "source": [
    "# === 1. Load and expand your JSON lines dataset ===\n",
    "file_path = \"data/train_v2.jsonl\"  # <-- put your dataset filename here\n",
    "\n",
    "rows = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        text = item[\"text\"]\n",
    "        acronym = item[\"acronym\"]\n",
    "        options = item[\"options\"]\n",
    "\n",
    "        for option_text, is_correct in options.items():\n",
    "            rows.append({\n",
    "                \"text\": text.strip(),\n",
    "                \"acronym\": acronym.strip(),\n",
    "                \"option_text\": option_text.strip(),\n",
    "                \"label\": int(is_correct)\n",
    "            })\n",
    "\n",
    "# Optional: check what it looks like\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77d1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Convert into a Hugging Face Dataset ===\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856bf8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2177/2177 [00:00<00:00, 5284.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# === 3. Tokenize ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")  # French-friendly model\n",
    "\n",
    "def preprocess(example):\n",
    "    # map(..., batched=True) provides lists — build a single input string per item\n",
    "    texts = example[\"text\"]\n",
    "    acronyms = example[\"acronym\"]\n",
    "    options = example[\"option_text\"]\n",
    "\n",
    "    if not isinstance(texts, list):\n",
    "        texts = [texts]\n",
    "        acronyms = [acronyms]\n",
    "        options = [options]\n",
    "\n",
    "    inputs = [t.strip() + \" \" + a.strip() + \" : \" + o.strip()\n",
    "              for t, a, o in zip(texts, acronyms, options)]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    # keep labels under key 'labels' for Trainer\n",
    "    tokenized[\"labels\"] = example[\"label\"]\n",
    "    return tokenized\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Split into train/validation sets (e.g., 90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b4166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# === 4. Initialize model ===\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c1a314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: TrainingArguments raised TypeError when using 'evaluation_strategy'.\n",
      "Falling back to older-compatible arguments (omitting evaluation_strategy).\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bdosanjosg\\AppData\\Local\\Temp\\ipykernel_5420\\709568092.py\", line 5, in <module>\n",
      "    training_args = TrainingArguments(\n",
      "TypeError: __init__() got an unexpected keyword argument 'evaluation_strategy'\n"
     ]
    }
   ],
   "source": [
    "# === 5. Training configuration ===\n",
    "# Some transformer versions don't accept newer kwargs (e.g., evaluation_strategy).\n",
    "# Try the modern constructor first; if it fails (TypeError), fall back to a compatible set.\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "except TypeError as e:\n",
    "    # Likely an older transformers version where evaluation_strategy is not supported.\n",
    "    print(\"Warning: TrainingArguments raised TypeError when using 'evaluation_strategy'.\")\n",
    "    print(\"Falling back to older-compatible arguments (omitting evaluation_strategy).\")\n",
    "    # Optionally show the original error for debugging\n",
    "    traceback.print_exception(e, e, e.__traceback__, file=sys.stdout)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        do_eval=True  # older flag that may be recognized\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe875a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Trainer setup ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Train ===\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbac9282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7028547525405884, 'eval_model_preparation_time': 0.005, 'eval_runtime': 17.0366, 'eval_samples_per_second': 12.796, 'eval_steps_per_second': 1.644}\n"
     ]
    }
   ],
   "source": [
    "# Print final metrics\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb6eefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results\\\\tokenizer_config.json',\n",
       " './results\\\\special_tokens_map.json',\n",
       " './results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 8. Save final model and tokenizer ===\n",
    "trainer.save_model(\"./results\")        # Saves model + config\n",
    "tokenizer.save_pretrained(\"./results\") # Saves tokenizer files too\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
