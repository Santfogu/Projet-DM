{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edebeca6-f9fd-4a7c-9640-fdde211c4d07",
   "metadata": {},
   "source": [
    "# Relevant examples with embeddings\n",
    "## Precomputing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309afe68-3f3d-4228-a017-67eea2e297b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_json('./data/train_v2.jsonl', lines=True)\n",
    "test_df = pd.read_json('./data/test_v4.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfaf235f-3407-4d0f-b281-db0dcac8e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n",
      "Batches: 100%|██████████| 17/17 [00:12<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "embedder = SentenceTransformer(\"intfloat/multilingual-e5-base\", device=\"cpu\")\n",
    "\n",
    "train_embs = embedder.encode(\n",
    "    train_df['text'].tolist(),\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "test_embs = embedder.encode(\n",
    "    test_df['text'].tolist(),\n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "del embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5ad20-7152-44be-b729-8f04e8f8a7d6",
   "metadata": {},
   "source": [
    "Precompute top-k indices for each test row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1474b14e-f8c7-425a-a659-772c9ec6a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "topk_indices = []\n",
    "\n",
    "for q_idx, row in test_df.iterrows():\n",
    "    acronym = row['acronym']\n",
    "    # Filter same acronym in train_df\n",
    "    subdf = train_df[train_df['acronym'] == acronym]\n",
    "    if len(subdf) == 0:\n",
    "    # fallback: random examples\n",
    "        topk_indices.append(train_df.sample(k).index.to_list())\n",
    "        continue\n",
    "\n",
    "    subset_indices = subdf.index.to_list()\n",
    "    subset_embs = train_embs[subset_indices]\n",
    "    \n",
    "    # Cosine similarity (dot product of normalized embeddings)\n",
    "    sims = np.dot(subset_embs, test_embs[q_idx].reshape(-1,1)).squeeze()\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_indices.append([subset_indices[i] for i in topk_idx])\n",
    "\n",
    "test_df['topk_example_indices'] = topk_indices\n",
    "\n",
    "del test_embs, train_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbed42-8177-4e08-bc53-e1805f0f3139",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Loading LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b66e842-31e4-4d86-9192-22dd2d8eb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.000000GB\n",
      "torch.cuda.memory_reserved: 0.000000GB\n",
      "torch.cuda.max_memory_reserved: 0.000000GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65af560e-b19b-45da-ae09-6b3e5ef39974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 17 files:   6%|▌         | 1/17 [05:10<1:22:51, 310.70s/it]'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-32B-Instruct/resolve/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/model-00014-of-00017.safetensors (Caused by ProxyError(\\'Unable to connect to proxy\\', NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f4b05f86780>: Failed to resolve \\'proxy.univ-lyon1.fr\\' ([Errno -3] Temporary failure in name resolution)\")))'), '(Request ID: d0509f0f-4c76-46b0-b725-0c639c3381f4)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-32B-Instruct/resolve/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd/model-00014-of-00017.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Fetching 17 files: 100%|██████████| 17/17 [08:29<00:00, 29.95s/it]  \n",
      "Loading checkpoint shards: 100%|██████████| 17/17 [13:41<00:00, 48.33s/it]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-32B-Instruct\" #\"mistralai/Mistral-Small-24B-Instruct-2501\" #\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "pipe.model.config.use_cache = False\n",
    "\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff0ba6-21e8-4756-a0ff-64b5f69a9f1f",
   "metadata": {},
   "source": [
    "Building prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd7fc04-c609-4c4d-8264-24055e50f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_with_examples(acronym, text, options, k=5):\n",
    "    system = \"\"\"Tu es un modèle expert en expansion d'acronymes ferroviaires.\n",
    "Ton rôle est d'identifier la ou les définitions correctes d'un acronyme dans un texte.\n",
    "Réponds uniquement avec une liste Python d'indices, ex. [0] ou [1, 2] ou []. \\nExemples:\"\"\"\n",
    "    for idx in row['topk_example_indices']: \n",
    "        example = train_df.iloc[idx]\n",
    "        system += f'\\nTexte exemple : \"{example[\"text\"]}\\nAcronyme: {example[\"acronym\"]}\"\\nOptions: '\n",
    "        for j, opt in enumerate(example['options'].keys()):\n",
    "            system += f'\\n{j}. : {opt}'\n",
    "        system += f'\\nReponse correcte : {[i for i, value in enumerate(example[\"options\"].values()) if value]}\\n'\n",
    "    user = f'Texte : \"{text}\"\\nAcronyme : {acronym}\\n'\n",
    "    for i, opt in enumerate(options):\n",
    "        user += f\"Option {i} : {opt}\\n\"\n",
    "    user += \"Réponds avec la liste des numéros corrects :\"\n",
    "    prompt = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": user}]\n",
    "    chat = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2872e07-49ec-4730-8b98-1e69b1a2acc1",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e704ad58-d2c4-4428-bbec-12013b229cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "for _, row in test_df.iterrows():\n",
    "    prompt = create_prompt_with_examples(row['acronym'], row['text'], row['options'])\n",
    "    inputs.append(prompt)\n",
    "\n",
    "outputs = pipe(inputs, do_sample=False, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ba7431-0a5a-4ddb-abd9-b8e161bf95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_predicted_ids(outputs):\n",
    "    predicted_ids = []\n",
    "    for output in outputs:\n",
    "        # Get the text after [/INST]\n",
    "        text = output[0][\"generated_text\"].split(\"<|im_start|>assistant\")[1] #\"[/INST]\"\n",
    "        \n",
    "        # Find all numeric patterns, including decimals\n",
    "        numbers = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "        \n",
    "        # Convert to ints safely, remove duplicates, and filter < 15\n",
    "        ids = list(set(int(float(i)) for i in numbers if float(i) < 15))\n",
    "        \n",
    "        predicted_ids.append(ids)\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d28c92be-085b-4c79-a0c1-4b7e41726247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.6513\n"
     ]
    }
   ],
   "source": [
    "predicted_ids = extract_predicted_ids(outputs)\n",
    "submission = pd.DataFrame({\"id\": test_df.index, \"prediction\":predicted_ids})\n",
    "submission.head()\n",
    "submission.to_csv(\"./predictions/rag2.csv\", index=False)\n",
    "\n",
    "previous = pd.read_csv(\"./predictions/rag-mistral-small.csv\")\n",
    "\n",
    "merged = previous.merge(submission, on=\"id\", suffixes=(\"_prev\", \"_new\"))\n",
    "merged['prediction_prev'] = merged['prediction_prev'].astype(str).str.strip()\n",
    "merged['prediction_new'] = merged['prediction_new'].astype(str).str.strip()\n",
    "\n",
    "similarity = (merged['prediction_prev'] == merged['prediction_new']).mean()\n",
    "print(f\"similarity: {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
