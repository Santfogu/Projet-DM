{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624146af-98ff-451d-8d08-86356163cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_pickle('train_df_with_embeddings.pkl')\n",
    "test_df = pd.read_pickle('test_df_with_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bceafa17-c32b-477c-9067-e23bfbf9ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_embs = np.stack(train_df['embeddings'].values)\n",
    "test_embs = np.stack(test_df['embeddings'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8255e245-8273-48db-8240-5d257feb03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "topk_indices = []\n",
    "\n",
    "for q_idx, row in test_df.iterrows():\n",
    "    acronym = row['acronym']\n",
    "    # Filter same acronym in train_df\n",
    "    subdf = train_df[train_df['acronym'] == acronym]\n",
    "    if len(subdf) == 0:\n",
    "        # fallback: random examples\n",
    "        topk_indices.append(train_df.sample(k).index.to_list())\n",
    "        continue\n",
    "\n",
    "    subset_indices = subdf.index.to_list()\n",
    "    subset_embs = train_embs[subset_indices]\n",
    "    \n",
    "    # Cosine similarity (dot product of normalized embeddings)\n",
    "    sims = np.dot(subset_embs, test_embs[q_idx].reshape(-1,1)).squeeze()\n",
    "    topk_idx = np.argsort(-sims)[:k]\n",
    "    topk_indices.append([subset_indices[i] for i in topk_idx])\n",
    "\n",
    "test_df['topk_example_indices'] = topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f174caea-c699-4f2d-927a-78a9e7263f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 10/10 [09:46<00:00, 58.62s/it]\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"mistralai/Mistral-Small-24B-Instruct-2501\" #\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "pipe.model.config.use_cache = False\n",
    "\n",
    "print(\"model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18072594-304c-4597-900b-bbe112a6318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_with_tot(acronym, text, options, examples):\n",
    "    system = \"\"\"Tu es un modèle expert en expansion d'acronymes ferroviaires.\n",
    "Ton rôle est d'identifier la ou les définitions correctes d'un acronyme dans un texte. Analyse synthètiquement le text et les options. \n",
    "Après termine en ecrivant les indeces des accronymes corrects sous la forme d'une liste de python. \\nExemples:\"\"\"\n",
    "\n",
    "    for idx in examples: \n",
    "        example = train_df.iloc[idx]\n",
    "        system += f'\\nTexte exemple : \"{example[\"text\"]}\\nAcronyme: {example[\"acronym\"]}\"\\nOptions: '\n",
    "        for j, opt in enumerate(example['options'].keys()):\n",
    "            system += f'\\n{j}. : {opt}'\n",
    "        system += f'\\nReponse correcte : {[i for i, value in enumerate(example[\"options\"].values()) if value]}\\n'\n",
    "    user = f'Texte : \"{text}\"\\nAcronyme : {acronym}\\n'\n",
    "    for i, opt in enumerate(options):\n",
    "        user += f\"Option {i} : {opt}\\n\"\n",
    "    user += \"Analyse chaqu'une des options, termine avec une réponse pour chaque option sous le format indiqué\"\n",
    "    prompt = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": user}]\n",
    "    chat = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    return chat\n",
    "\n",
    "#create_prompt_with_tot(test.iloc[0]['acronym'], test.iloc[0]['text'], test.iloc[0]['options'].keys(), top_k_examples(5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ac455f-0abd-4f9e-9139-448e60d638d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_predicted_ids_tot(outputs):\n",
    "    predicted_ids = []\n",
    "    for output in outputs:\n",
    "        # Get the text after [/INST]\n",
    "        text = output[0][\"generated_text\"].split(\"[/INST]\")[1] #\"[/INST]\"\n",
    "        \n",
    "        ids_for_this_output = []\n",
    " \n",
    "        bracket_contents = re.findall(r'\\[(.*?)\\]', text)\n",
    "        \n",
    "        for content in bracket_contents:\n",
    "            # Find all numbers within each bracket content\n",
    "            numbers = re.findall(r'\\d+', content)\n",
    "        \n",
    "        # Convert to ints safely, remove duplicates, and filter < 15\n",
    "        ids = list(set(int(float(i)) for i in numbers if float(i) < 15))\n",
    "        \n",
    "        predicted_ids.append(ids)\n",
    "    return predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5612096-cc55-4074-b32c-e315f3250071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tot(k=4):\n",
    "    inputs = []\n",
    "    for indexx, row in test_df.iterrows():\n",
    "        prompt = create_prompt_with_tot(row['acronym'], row['text'], row['options'], row['topk_example_indices'])\n",
    "        inputs.append(prompt)\n",
    "    outputs = pipe(inputs, temperature=0, max_new_tokens=768, do_sample=False, batch_size=4)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58cf96-9e12-4cd9-8a61-cc60f034580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = predict_with_tot(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24b2fb-fc98-4650-80cf-69eceb213bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ids = extract_predicted_ids_tot(outputs)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": test_df.index, \"prediction\":predicted_ids})\n",
    "submission.head()\n",
    "submission.to_csv(\"./predictions/rag-tot-2.csv\", index=False)\n",
    "\n",
    "previous = pd.read_csv(\"./predictions/rag-tot.csv\")\n",
    "\n",
    "merged = previous.merge(submission, on=\"id\", suffixes=(\"_prev\", \"_new\"))\n",
    "merged['prediction_prev'] = merged['prediction_prev'].astype(str).str.strip()\n",
    "merged['prediction_new'] = merged['prediction_new'].astype(str).str.strip()\n",
    "\n",
    "similarity = (merged['prediction_prev'] == merged['prediction_new']).mean()\n",
    "print(f\"similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bbf54-903d-4af2-96de-9cb3f0c4587b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
