{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0414f6d9",
   "metadata": {},
   "source": [
    "# Projet DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf31586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\santi\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\santi\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     ----------------- ---------------------- 7.1/16.3 MB 43.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.3/16.3 MB 48.8 MB/s  0:00:00\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d840f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    \"\"\"Load a .jsonl file. Tries both the given path and the 'data' folder.\n",
    "    Returns a list of dicts.\"\"\"\n",
    "    p = os.path.join('data', filename)\n",
    "    if os.path.exists(p):\n",
    "        data = []\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing a line in {p}: {e}\")\n",
    "        return data\n",
    "    raise FileNotFoundError(f\"File not found: {filename} (tried: {p})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04bdd7",
   "metadata": {},
   "source": [
    "### 1. Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c64e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train records: 492, Test records: 519\n",
      "-- Train head --\n",
      "                                                text acronym  \\\n",
      "0  LRA  limite de r√©sistance des attelages PAR po...     PAR   \n",
      "1                              D√©signa -tion des PN       PN   \n",
      "2  pr√©d√©termin√©es de trains : _x0001_ les masses ...      EM   \n",
      "3  /Commentaires N¬∞ AC B81500 thermique:  compati...      AC   \n",
      "4  kilom√®tres/heure (ex : 12 pour 120 km/h), _x00...     TIV   \n",
      "\n",
      "                                             options  \n",
      "0  {'Plan d'action r√©gularit√©': False, 'Poste d'a...  \n",
      "1  {'Passages √† niveau : fichier des pn, recensem...  \n",
      "2  {'EMERAINVILLE PONTAULT COMBAULT': False, 'Eng...  \n",
      "3  {'ACc√®s': False, 'Agent d'aCcompagnement ': Fa...  \n",
      "4  {'THIVIERS': False, 'Trafic international voya...  \n",
      "\n",
      "-- Info --\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 492 entries, 0 to 491\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     492 non-null    object\n",
      " 1   acronym  492 non-null    object\n",
      " 2   options  492 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "train_path = 'train_v2.jsonl'\n",
    "test_path = 'test_v4.jsonl'\n",
    "\n",
    "train_data = load_jsonl(train_path)\n",
    "test_data = load_jsonl(test_path)\n",
    "\n",
    "print(f\"Train records: {len(train_data)}, Test records: {len(test_data)}\")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Quick preview\n",
    "print(\"-- Train head --\")\n",
    "print(train_df.head())\n",
    "print(\"\\n-- Info --\")\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccd4a2",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784b460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['text', 'acronym', 'options']\n",
      "Missing values per column:\n",
      " text       0\n",
      "acronym    0\n",
      "options    0\n",
      "dtype: int64\n",
      "After cleaning, shape: (492, 3)\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning and review\n",
    "# 1) Columns, types, and missing values\n",
    "print(\"Columns:\", list(train_df.columns))\n",
    "print(\"Missing values per column:\\n\", train_df.isna().sum())\n",
    "\n",
    "# 2) Remove simple duplicates\n",
    "train_df = train_df.drop_duplicates(subset=['text', 'acronym']).reset_index(drop=True)\n",
    "\n",
    "# 3) Fill missing values in text columns with empty string (example)\n",
    "for col in train_df.select_dtypes(include='object').columns:\n",
    "    train_df[col] = train_df[col].fillna('')\n",
    "\n",
    "print(\"After cleaning, shape:\", train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac1be1",
   "metadata": {},
   "source": [
    "### 3. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b06ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading stopwords-fr: Package 'stopwords-fr' not\n",
      "[nltk_data]     found in index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando textos de train...\n",
      "Procesando textos de test...\n",
      "Procesando opciones de train...\n",
      "Procesando opciones de test...\n",
      "‚úÖ Preprocesamiento completado\n",
      "Ejemplo de texto procesado: lra limite resistance attelage poste regulation pl pleine lign pn passage avoir niveau rfn reseau fe...\n",
      "Ejemplo de opciones procesadas: ['plan regularite', 'poste et de regulation assurer le commande de installation de signalisation et le gestion de le circulation de huit ligne avoir grand vitesse', 'pont de', 'plan regional']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK (solo una vez)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"stopwords-fr\")\n",
    "\n",
    "# Cargar modelo de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Obtener stopwords en franc√©s\n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "def preprocessing(text, options=False):\n",
    "    \"\"\"\n",
    "    Funci√≥n unificada de preprocesamiento que aplica los mismos pasos a texto y opciones\n",
    "    \"\"\"\n",
    "    # Verificar si el texto es NaN o None\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Convertir a string y lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. Quitar acentos\n",
    "    text = unidecode(text)\n",
    "    \n",
    "    # 3. Tokenizar y quitar stopwords\n",
    "    try:\n",
    "        tokens = word_tokenize(text, language='french')\n",
    "    except:\n",
    "        # Fallback para tokenizaci√≥n simple si word_tokenize falla\n",
    "        tokens = text.split()\n",
    "    \n",
    "    if options:\n",
    "        # PARA OPCIONES: mantener stopwords\n",
    "        filtered_tokens = [word for word in tokens if word.isalpha()]\n",
    "    else:\n",
    "        # PARA TEXTO NORMAL: quitar stopwords  \n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    # 4. Reconstruir texto para lematizaci√≥n\n",
    "    text_to_lemmatize = \" \".join(filtered_tokens)\n",
    "    \n",
    "    # 5. Lemmatizar con spaCy\n",
    "    try:\n",
    "        doc = nlp(text_to_lemmatize)\n",
    "        lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "        return \" \".join(lemmas)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en lematizaci√≥n: {e}\")\n",
    "        return text_to_lemmatize  # Fallback sin lematizaci√≥n\n",
    "\n",
    "# Aplicar a los textos principales\n",
    "print(\"Procesando textos de train...\")\n",
    "train_df['text_processed'] = train_df['text'].apply(preprocessing)\n",
    "print(\"Procesando textos de test...\")\n",
    "test_df['text_processed'] = test_df['text'].apply(preprocessing)\n",
    "\n",
    "# Funciones para procesar opciones\n",
    "def process_options_dict(options_dict):\n",
    "    \"\"\"Procesar opciones del training (diccionario)\"\"\"\n",
    "    processed = {}\n",
    "    for key, value in options_dict.items():\n",
    "        processed_key = preprocessing(key,True)\n",
    "        processed[processed_key] = value\n",
    "    return processed\n",
    "\n",
    "def process_options_list(options_list):\n",
    "    \"\"\"Procesar opciones del test (lista)\"\"\"\n",
    "    return [preprocessing(opt,True) for opt in options_list]\n",
    "\n",
    "# Aplicar a las opciones\n",
    "print(\"Procesando opciones de train...\")\n",
    "train_df['options_processed'] = train_df['options'].apply(process_options_dict)\n",
    "print(\"Procesando opciones de test...\")\n",
    "test_df['options_processed'] = test_df['options'].apply(process_options_list)\n",
    "\n",
    "print(\"‚úÖ Preprocesamiento completado\")\n",
    "print(f\"Ejemplo de texto procesado: {train_df['text_processed'].iloc[0][:100]}...\")\n",
    "print(f\"Ejemplo de opciones procesadas: {list(train_df['options_processed'].iloc[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec0850",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049daaf1",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a6f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partir datos en train y validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Partir datos en train y validation\n",
    "train_final, val_final = train_test_split(train_df, test_size=0.3, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a39c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construyendo vocabulario TF-IDF...\n",
      "‚úÖ TF-IDF GLOBAL entrenado con 7425 t√©rminos\n"
     ]
    }
   ],
   "source": [
    "# === NUEVO: SISTEMA DE SIMILITUD ===\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Construyendo vocabulario TF-IDF...\")\n",
    "\n",
    "# Crear un corpus con TODOS los textos relevantes\n",
    "all_texts_for_tfidf = []\n",
    "\n",
    "# 1. Agregar textos principales de entrenamiento y prueba\n",
    "all_texts_for_tfidf.extend(train_final['text_processed'].tolist())\n",
    "\n",
    "# 2. Agregar TODAS las opciones preprocesadas (esto es clave)\n",
    "for options_dict in train_final['options_processed']:\n",
    "    all_texts_for_tfidf.extend(options_dict.keys())\n",
    "\n",
    "# 3. Agregar textos de test tambi√©n para mejor cobertura\n",
    "all_texts_for_tfidf.extend(test_df['text_processed'].tolist())\n",
    "\n",
    "# 4. Entrenar TF-IDF GLOBAL con par√°metros mejorados\n",
    "global_tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),  # Incluir bigramas y trigramas\n",
    "    min_df=2,            # Ignorar t√©rminos muy raros\n",
    "    max_features=10000   # Limitar vocabulario\n",
    ")\n",
    "global_tfidf.fit(all_texts_for_tfidf)\n",
    "\n",
    "print(f\"‚úÖ TF-IDF GLOBAL entrenado con {len(global_tfidf.vocabulary_)} t√©rminos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbbb845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>acronym</th>\n",
       "      <th>options</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>options_processed</th>\n",
       "      <th>answer_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>VENISSIEUX √† AMBERIEU  Article B101 Domaine de...</td>\n",
       "      <td>EF</td>\n",
       "      <td>{'Entreprise Ferroviaire :   Toute entreprise ...</td>\n",
       "      <td>venissieux avoir amberieu article domaine circ...</td>\n",
       "      <td>{'entreprise ferroviaire tout entreprise avoir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Boingneville PLBV ‚Äì Poste 1MALESHERBES(1) ‚Äì Et...</td>\n",
       "      <td>BV</td>\n",
       "      <td>{'Bassin Versant': False, 'B√¢timent des Voyage...</td>\n",
       "      <td>boingnevill plbv poste etablissement pl suscep...</td>\n",
       "      <td>{'bassin verser': False, 'batiment de voyageur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Feuqui√®res Fressenneville - PL 30Woincourt - P...</td>\n",
       "      <td>PL</td>\n",
       "      <td>{'Panneaux lumineux': False, 'Pleine Ligne.   ...</td>\n",
       "      <td>feuquiere fressennevill pl pl kilom√®tre ligne ...</td>\n",
       "      <td>{'panneau lumineux': False, 'pleine lign etabl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>franchissement des signaux d'arr√™t Ligne √©quip...</td>\n",
       "      <td>DAAT</td>\n",
       "      <td>{'Diagnostique Amiante Avant Travaux': False, ...</td>\n",
       "      <td>franchissemer signal ligne equipe dispositif a...</td>\n",
       "      <td>{'diagnostique amiant avant travail': False, '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Page 4 NPDC-RT-2211B- Version 01 du 15-12-2014...</td>\n",
       "      <td>EF</td>\n",
       "      <td>{'Equipement fixe': False, 'Essai de Frein': F...</td>\n",
       "      <td>page version article domaine circulation appar...</td>\n",
       "      <td>{'equipement fixe': False, 'essai de frein': F...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text acronym  \\\n",
       "184  VENISSIEUX √† AMBERIEU  Article B101 Domaine de...      EF   \n",
       "252  Boingneville PLBV ‚Äì Poste 1MALESHERBES(1) ‚Äì Et...      BV   \n",
       "397  Feuqui√®res Fressenneville - PL 30Woincourt - P...      PL   \n",
       "325  franchissement des signaux d'arr√™t Ligne √©quip...    DAAT   \n",
       "136  Page 4 NPDC-RT-2211B- Version 01 du 15-12-2014...      EF   \n",
       "\n",
       "                                               options  \\\n",
       "184  {'Entreprise Ferroviaire :   Toute entreprise ...   \n",
       "252  {'Bassin Versant': False, 'B√¢timent des Voyage...   \n",
       "397  {'Panneaux lumineux': False, 'Pleine Ligne.   ...   \n",
       "325  {'Diagnostique Amiante Avant Travaux': False, ...   \n",
       "136  {'Equipement fixe': False, 'Essai de Frein': F...   \n",
       "\n",
       "                                        text_processed  \\\n",
       "184  venissieux avoir amberieu article domaine circ...   \n",
       "252  boingnevill plbv poste etablissement pl suscep...   \n",
       "397  feuquiere fressennevill pl pl kilom√®tre ligne ...   \n",
       "325  franchissemer signal ligne equipe dispositif a...   \n",
       "136  page version article domaine circulation appar...   \n",
       "\n",
       "                                     options_processed  answer_index  \n",
       "184  {'entreprise ferroviaire tout entreprise avoir...             0  \n",
       "252  {'bassin verser': False, 'batiment de voyageur...             1  \n",
       "397  {'panneau lumineux': False, 'pleine lign etabl...            -1  \n",
       "325  {'diagnostique amiant avant travail': False, '...             1  \n",
       "136  {'equipement fixe': False, 'essai de frein': F...             2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_final.head()\n",
    "#Add a new column with an int being the index of the correct answer among the options\n",
    "def get_answer_index(row):\n",
    "    options = row['options_processed']\n",
    "    for idx, option in enumerate(options):\n",
    "        if options[option] == True:\n",
    "            return idx\n",
    "    return -1  # En caso de no encontrar\n",
    "val_final['answer_index'] = val_final.apply(get_answer_index, axis=1)\n",
    "val_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1ae1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDACI√ìN COMPLETA CON M√öLTIPLES PAR√ÅMETROS ===\n",
      "\n",
      "1. PROBANDO DIFERENTES THRESHOLDS:\n",
      "  Threshold 0.000: Accuracy=0.5068, Confianza_promedio=0.0606, Cobertura=0.7500\n",
      "  Threshold 0.001: Accuracy=0.5068, Confianza_promedio=0.0606, Cobertura=0.7500\n",
      "  Threshold 0.005: Accuracy=0.5068, Confianza_promedio=0.0606, Cobertura=0.7500\n",
      "  Threshold 0.010: Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Threshold 0.020: Accuracy=0.4595, Confianza_promedio=0.0606, Cobertura=0.6554\n",
      "  Threshold 0.050: Accuracy=0.3581, Confianza_promedio=0.0606, Cobertura=0.4392\n",
      "  Threshold 0.100: Accuracy=0.2973, Confianza_promedio=0.0606, Cobertura=0.1959\n",
      "  Threshold 0.150: Accuracy=0.2905, Confianza_promedio=0.0606, Cobertura=0.1149\n",
      "  Threshold 0.200: Accuracy=0.2635, Confianza_promedio=0.0606, Cobertura=0.0676\n",
      "  Threshold 0.250: Accuracy=0.2568, Confianza_promedio=0.0606, Cobertura=0.0473\n",
      "\n",
      "2. PROBANDO DIFERENTES N-GRAMAS:\n",
      "  ngram_range (1, 1): Accuracy=0.5068, Confianza_promedio=0.0903, Cobertura=0.7500\n",
      "  ngram_range (1, 2): Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  ngram_range (1, 3): Accuracy=0.4797, Confianza_promedio=0.0468, Cobertura=0.7095\n",
      "  ngram_range (2, 2): Accuracy=0.3851, Confianza_promedio=0.0192, Cobertura=0.2432\n",
      "  ngram_range (2, 3): Accuracy=0.3851, Confianza_promedio=0.0127, Cobertura=0.2365\n",
      "  ngram_range (3, 3): Accuracy=0.2635, Confianza_promedio=0.0040, Cobertura=0.0270\n",
      "\n",
      "3. PROBANDO ESTRATEGIAS DE CONSULTA:\n",
      "  Query 'text_only': Accuracy=0.5135, Confianza_promedio=0.0568, Cobertura=0.7027\n",
      "  Query 'acronym_only': Accuracy=0.2905, Confianza_promedio=0.0724, Cobertura=0.2432\n",
      "  Query 'text_acronym': Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Query 'acronym_text': Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Query 'weighted_text': Accuracy=0.4932, Confianza_promedio=0.0584, Cobertura=0.7297\n",
      "\n",
      "4. PROBANDO ESTRATEGIAS DE FALLBACK:\n",
      "  Fallback 'first': Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Fallback 'random': Accuracy=0.5338, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Fallback 'last': Accuracy=0.5135, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Fallback 'middle': Accuracy=0.5203, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "\n",
      "5. COMBINACIONES PROMETEDORAS:\n",
      "  Combo 1: Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "    Par√°metros: {'threshold': 0.01, 'ngram_range': (1, 2), 'query_strategy': 'text_acronym'}\n",
      "  Combo 2: Accuracy=0.3108, Confianza_promedio=0.0447, Cobertura=0.2703\n",
      "    Par√°metros: {'threshold': 0.05, 'ngram_range': (1, 3), 'query_strategy': 'weighted_text'}\n",
      "  Combo 3: Accuracy=0.4865, Confianza_promedio=0.0568, Cobertura=0.6351\n",
      "    Par√°metros: {'threshold': 0.02, 'ngram_range': (1, 2), 'query_strategy': 'text_only'}\n",
      "  Combo 4: Accuracy=0.2635, Confianza_promedio=0.0127, Cobertura=0.0405\n",
      "    Par√°metros: {'threshold': 0.1, 'ngram_range': (2, 3), 'query_strategy': 'acronym_text'}\n",
      "\n",
      "6. B√öSQUEDA FINA DE THRESHOLD:\n",
      "  Threshold 0.005: Accuracy=0.5068, Confianza_promedio=0.0606, Cobertura=0.7500\n",
      "  Threshold 0.008: Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7432\n",
      "  Threshold 0.010: Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Threshold 0.012: Accuracy=0.5000, Confianza_promedio=0.0606, Cobertura=0.7230\n",
      "  Threshold 0.015: Accuracy=0.4730, Confianza_promedio=0.0606, Cobertura=0.6824\n",
      "  Threshold 0.018: Accuracy=0.4730, Confianza_promedio=0.0606, Cobertura=0.6689\n",
      "  Threshold 0.020: Accuracy=0.4595, Confianza_promedio=0.0606, Cobertura=0.6554\n",
      "  Threshold 0.025: Accuracy=0.4392, Confianza_promedio=0.0606, Cobertura=0.6014\n",
      "\n",
      "üéØ MEJOR THRESHOLD: 0.005 con accuracy: 0.5068\n",
      "\n",
      "7. B√öSQUEDA ALEATORIA DE PAR√ÅMETROS (CORREGIDA):\n",
      "üî• Nuevo mejor: Accuracy=0.5000, threshold=0.001, ngram_range=(1, 2), query_strategy='weighted_text'\n",
      "üî• Nuevo mejor: Accuracy=0.5068, threshold=0.005, ngram_range=(1, 2), query_strategy='text_acronym'\n",
      "\n",
      "üèÜ MEJORES PAR√ÅMETROS ENCONTRADOS:\n",
      "  threshold: 0.005\n",
      "  ngram_range: (1, 2)\n",
      "  query_strategy: text_acronym\n",
      "  accuracy: 0.5067567567567568\n",
      "\n",
      "8. EVALUACI√ìN FINAL CON MEJORES PAR√ÅMETROS:\n",
      "‚úÖ Resultado final: Accuracy=0.5068\n"
     ]
    }
   ],
   "source": [
    "# Validation con m√∫ltiples par√°metros\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def improved_similarity_prediction_validation(df, threshold=0.01, ngram_range=(1, 2), \n",
    "                                            use_acronym=True, query_strategy=\"text_acronym\",\n",
    "                                            fallback_strategy=\"first\"):\n",
    "    \"\"\"\n",
    "    Predicci√≥n mejorada con m√∫ltiples par√°metros ajustables\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de validaci√≥n\n",
    "        threshold: Umbral de similitud m√≠nima\n",
    "        ngram_range: Rango de n-gramas para TF-IDF\n",
    "        use_acronym: Si incluir el acr√≥nimo en la consulta\n",
    "        query_strategy: Estrategia para construir la consulta\n",
    "        fallback_strategy: Estrategia cuando no supera el threshold\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    validation_scores = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            text = row['text_processed']\n",
    "            acronym = row['acronym']\n",
    "            options = row['options_processed']\n",
    "            \n",
    "            # Diferentes estrategias para construir la consulta\n",
    "            if query_strategy == \"text_only\":\n",
    "                query = text\n",
    "            elif query_strategy == \"acronym_only\":\n",
    "                query = acronym\n",
    "            elif query_strategy == \"text_acronym\":\n",
    "                query = f\"{text} {acronym}\"\n",
    "            elif query_strategy == \"acronym_text\":\n",
    "                query = f\"{acronym} {text}\"\n",
    "            elif query_strategy == \"weighted_text\":\n",
    "                # Dar m√°s peso al texto repiti√©ndolo\n",
    "                query = f\"{text} {text} {acronym}\"\n",
    "            else:\n",
    "                query = f\"{text} {acronym}\"\n",
    "            \n",
    "            # Crear vectorizador TF-IDF con par√°metros ajustables\n",
    "            vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "            \n",
    "            # Since options in validation are in dict format, we need to get the keys\n",
    "            options_list = list(options.keys())\n",
    "            corpus = [query] + options_list\n",
    "            tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "            \n",
    "            # Calcular similitudes (query est√° en posici√≥n 0)\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "            \n",
    "            # Encontrar la opci√≥n m√°s similar\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_score = similarities[best_idx]\n",
    "            \n",
    "            confidence_scores.append(best_score)\n",
    "            \n",
    "            if best_score > threshold:\n",
    "                # Check if the predicted option is indeed the correct one\n",
    "                if best_idx == row['answer_index']:\n",
    "                    validation_scores.append(1)\n",
    "                else:\n",
    "                    validation_scores.append(0)\n",
    "                predictions.append([best_idx])\n",
    "            else:\n",
    "                # Diferentes estrategias de fallback\n",
    "                if fallback_strategy == \"first\":\n",
    "                    fallback_idx = 0\n",
    "                elif fallback_strategy == \"random\":\n",
    "                    fallback_idx = np.random.randint(0, len(options_list))\n",
    "                elif fallback_strategy == \"last\":\n",
    "                    fallback_idx = len(options_list) - 1\n",
    "                elif fallback_strategy == \"middle\":\n",
    "                    fallback_idx = len(options_list) // 2\n",
    "                else:\n",
    "                    fallback_idx = 0\n",
    "                \n",
    "                predictions.append([fallback_idx])\n",
    "                if fallback_idx == row['answer_index']:\n",
    "                    validation_scores.append(1)\n",
    "                else:\n",
    "                    validation_scores.append(0)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error en fila {idx}: {e}\")\n",
    "            # Fallback seguro\n",
    "            predictions.append([0])\n",
    "            validation_scores.append(0)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = np.mean(validation_scores) if validation_scores else 0\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    coverage = np.mean([1 if score > threshold else 0 for score in confidence_scores])\n",
    "    \n",
    "    return predictions, accuracy, avg_confidence, coverage\n",
    "\n",
    "# ===== PRUEBA DE DIFERENTES PAR√ÅMETROS =====\n",
    "\n",
    "print(\"=== VALIDACI√ìN COMPLETA CON M√öLTIPLES PAR√ÅMETROS ===\\n\")\n",
    "\n",
    "# 1. Prueba de diferentes thresholds\n",
    "print(\"1. PROBANDO DIFERENTES THRESHOLDS:\")\n",
    "thresholds = [0, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "for thresh in thresholds:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(\n",
    "        val_final, threshold=thresh)\n",
    "    print(f\"  Threshold {thresh:.3f}: Accuracy={accuracy:.4f}, \"\n",
    "          f\"Confianza_promedio={avg_conf:.4f}, Cobertura={coverage:.4f}\")\n",
    "\n",
    "# 2. Prueba de diferentes rangos de n-gramas\n",
    "print(\"\\n2. PROBANDO DIFERENTES N-GRAMAS:\")\n",
    "ngram_ranges = [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n",
    "for ngram in ngram_ranges:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(\n",
    "        val_final, ngram_range=ngram)\n",
    "    print(f\"  ngram_range {ngram}: Accuracy={accuracy:.4f}, \"\n",
    "          f\"Confianza_promedio={avg_conf:.4f}, Cobertura={coverage:.4f}\")\n",
    "\n",
    "# 3. Prueba de diferentes estrategias de consulta\n",
    "print(\"\\n3. PROBANDO ESTRATEGIAS DE CONSULTA:\")\n",
    "query_strategies = [\"text_only\", \"acronym_only\", \"text_acronym\", \"acronym_text\", \"weighted_text\"]\n",
    "for strategy in query_strategies:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(\n",
    "        val_final, query_strategy=strategy)\n",
    "    print(f\"  Query '{strategy}': Accuracy={accuracy:.4f}, \"\n",
    "          f\"Confianza_promedio={avg_conf:.4f}, Cobertura={coverage:.4f}\")\n",
    "\n",
    "# 4. Prueba de diferentes estrategias de fallback\n",
    "print(\"\\n4. PROBANDO ESTRATEGIAS DE FALLBACK:\")\n",
    "fallback_strategies = [\"first\", \"random\", \"last\", \"middle\"]\n",
    "for fallback in fallback_strategies:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(\n",
    "        val_final, fallback_strategy=fallback)\n",
    "    print(f\"  Fallback '{fallback}': Accuracy={accuracy:.4f}, \"\n",
    "          f\"Confianza_promedio={avg_conf:.4f}, Cobertura={coverage:.4f}\")\n",
    "\n",
    "# 5. Combinaciones prometedoras\n",
    "print(\"\\n5. COMBINACIONES PROMETEDORAS:\")\n",
    "combinations = [\n",
    "    {\"threshold\": 0.01, \"ngram_range\": (1, 2), \"query_strategy\": \"text_acronym\"},\n",
    "    {\"threshold\": 0.05, \"ngram_range\": (1, 3), \"query_strategy\": \"weighted_text\"},\n",
    "    {\"threshold\": 0.02, \"ngram_range\": (1, 2), \"query_strategy\": \"text_only\"},\n",
    "    {\"threshold\": 0.1, \"ngram_range\": (2, 3), \"query_strategy\": \"acronym_text\"},\n",
    "]\n",
    "\n",
    "for i, combo in enumerate(combinations):\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(\n",
    "        val_final, **combo)\n",
    "    print(f\"  Combo {i+1}: Accuracy={accuracy:.4f}, \"\n",
    "          f\"Confianza_promedio={avg_conf:.4f}, Cobertura={coverage:.4f}\")\n",
    "    print(f\"    Par√°metros: {combo}\")\n",
    "\n",
    "# 6. B√∫squeda m√°s fina alrededor del mejor threshold encontrado\n",
    "print(\"\\n6. B√öSQUEDA FINA DE THRESHOLD:\")\n",
    "fine_thresholds = [0.005, 0.008, 0.01, 0.012, 0.015, 0.018, 0.02, 0.025]\n",
    "best_threshold = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "for thresh in fine_thresholds:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(\n",
    "        val_final, threshold=thresh)\n",
    "    print(f\"  Threshold {thresh:.3f}: Accuracy={accuracy:.4f}, \"\n",
    "          f\"Confianza_promedio={avg_conf:.4f}, Cobertura={coverage:.4f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = thresh\n",
    "\n",
    "print(f\"\\nüéØ MEJOR THRESHOLD: {best_threshold} con accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Funci√≥n para encontrar los mejores par√°metros autom√°ticamente (CORREGIDA)\n",
    "def find_best_parameters(validation_df, n_trials=50):\n",
    "    \"\"\"Encuentra los mejores par√°metros mediante b√∫squeda aleatoria\"\"\"\n",
    "    best_params = {}\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    # Listas de par√°metros para seleccionar aleatoriamente\n",
    "    thresholds = [0, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "    ngram_ranges = [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3)]\n",
    "    query_strategies = [\"text_only\", \"acronym_only\", \"text_acronym\", \"weighted_text\"]\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        # Seleccionar par√°metros aleatorios usando √≠ndices\n",
    "        threshold = np.random.choice(thresholds)\n",
    "        ngram_range = ngram_ranges[np.random.randint(len(ngram_ranges))]\n",
    "        query_strategy = query_strategies[np.random.randint(len(query_strategies))]\n",
    "        \n",
    "        try:\n",
    "            _, accuracy, _, _ = improved_similarity_prediction_validation(\n",
    "                validation_df, \n",
    "                threshold=threshold,\n",
    "                ngram_range=ngram_range,\n",
    "                query_strategy=query_strategy\n",
    "            )\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {\n",
    "                    'threshold': threshold,\n",
    "                    'ngram_range': ngram_range,\n",
    "                    'query_strategy': query_strategy,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "                print(f\"üî• Nuevo mejor: Accuracy={accuracy:.4f}, threshold={threshold}, ngram_range={ngram_range}, query_strategy='{query_strategy}'\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en trial {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return best_params\n",
    "\n",
    "print(\"\\n7. B√öSQUEDA ALEATORIA DE PAR√ÅMETROS (CORREGIDA):\")\n",
    "best_params = find_best_parameters(val_final, n_trials=30)\n",
    "print(f\"\\nüèÜ MEJORES PAR√ÅMETROS ENCONTRADOS:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 8. Prueba adicional con los mejores par√°metros encontrados\n",
    "if best_params:\n",
    "    print(\"\\n8. EVALUACI√ìN FINAL CON MEJORES PAR√ÅMETROS:\")\n",
    "    final_predictions, final_accuracy, final_confidence, final_coverage = improved_similarity_prediction_validation(\n",
    "        val_final, \n",
    "        threshold=best_params['threshold'],\n",
    "        ngram_range=best_params['ngram_range'],\n",
    "        query_strategy=best_params['query_strategy']\n",
    "    )\n",
    "    print(f\"‚úÖ Resultado final: Accuracy={final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c0afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HACIENDO PREDICCIONES FINALES EN TEST ===\n",
      "Usando mejores par√°metros encontrados: {'threshold': 0.005, 'ngram_range': (1, 2), 'query_strategy': 'text_acronym', 'accuracy': 0.5067567567567568}\n",
      "‚úÖ Predicciones en test completadas:\n",
      "   - N√∫mero de predicciones: 519\n",
      "   - Confianza promedio: 0.0480\n",
      "   - Cobertura: 0.6686\n",
      "\n",
      "Distribuci√≥n de las respuestas predichas:\n",
      "answer\n",
      "[0]     255\n",
      "[1]     117\n",
      "[2]      64\n",
      "[3]      65\n",
      "[4]       6\n",
      "[5]       2\n",
      "[6]       3\n",
      "[7]       3\n",
      "[8]       1\n",
      "[9]       2\n",
      "[12]      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üéØ Archivo de submission guardado como: submission_improved.csv\n",
      "\n",
      "=== VALIDACI√ìN DEL ARCHIVO GUARDADO ===\n",
      "Archivo cargado: 519 filas\n",
      "Primeras 5 filas:\n",
      "   id answer\n",
      "0   0    [1]\n",
      "1   1    [1]\n",
      "2   2    [0]\n",
      "3   3    [1]\n",
      "4   4    [0]\n"
     ]
    }
   ],
   "source": [
    "### 6. PREDICCI√ìN FINAL EN TEST\n",
    "\n",
    "def improved_similarity_prediction_test(df, threshold=0.01, ngram_range=(1, 2), \n",
    "                                      query_strategy=\"text_acronym\", fallback_strategy=\"first\"):\n",
    "    \"\"\"\n",
    "    Predicci√≥n mejorada para el conjunto de test\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            text = row['text_processed']\n",
    "            acronym = row['acronym']\n",
    "            options = row['options_processed']  # En test es una lista\n",
    "            \n",
    "            # Construir la consulta seg√∫n la estrategia\n",
    "            if query_strategy == \"text_only\":\n",
    "                query = text\n",
    "            elif query_strategy == \"acronym_only\":\n",
    "                query = acronym\n",
    "            elif query_strategy == \"text_acronym\":\n",
    "                query = f\"{text} {acronym}\"\n",
    "            elif query_strategy == \"acronym_text\":\n",
    "                query = f\"{acronym} {text}\"\n",
    "            elif query_strategy == \"weighted_text\":\n",
    "                query = f\"{text} {text} {acronym}\"\n",
    "            else:\n",
    "                query = f\"{text} {acronym}\"\n",
    "            \n",
    "            # Crear vectorizador TF-IDF\n",
    "            vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "            \n",
    "            # En test, options es una lista, no un diccionario\n",
    "            options_list = options\n",
    "            corpus = [query] + options_list\n",
    "            tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "            \n",
    "            # Calcular similitudes (query est√° en posici√≥n 0)\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "            \n",
    "            # Encontrar la opci√≥n m√°s similar\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_score = similarities[best_idx]\n",
    "            \n",
    "            confidence_scores.append(best_score)\n",
    "            \n",
    "            if best_score > threshold:\n",
    "                predictions.append([best_idx])\n",
    "            else:\n",
    "                # Estrategia de fallback\n",
    "                if fallback_strategy == \"first\":\n",
    "                    fallback_idx = 0\n",
    "                elif fallback_strategy == \"random\":\n",
    "                    fallback_idx = np.random.randint(0, len(options_list))\n",
    "                elif fallback_strategy == \"last\":\n",
    "                    fallback_idx = len(options_list) - 1\n",
    "                elif fallback_strategy == \"middle\":\n",
    "                    fallback_idx = len(options_list) // 2\n",
    "                else:\n",
    "                    fallback_idx = 0\n",
    "                \n",
    "                predictions.append([fallback_idx])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error en fila {idx}: {e}\")\n",
    "            # Fallback seguro\n",
    "            predictions.append([0])\n",
    "            confidence_scores.append(0)\n",
    "    \n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    coverage = np.mean([1 if score > threshold else 0 for score in confidence_scores])\n",
    "    \n",
    "    return predictions, avg_confidence, coverage\n",
    "\n",
    "# Usar los mejores par√°metros encontrados para hacer las predicciones finales\n",
    "print(\"=== HACIENDO PREDICCIONES FINALES EN TEST ===\")\n",
    "\n",
    "if 'best_params' in locals() and best_params:\n",
    "    print(f\"Usando mejores par√°metros encontrados: {best_params}\")\n",
    "    \n",
    "    test_predictions, test_confidence, test_coverage = improved_similarity_prediction_test(\n",
    "        test_df,\n",
    "        threshold=best_params['threshold'],\n",
    "        ngram_range=best_params['ngram_range'],\n",
    "        query_strategy=best_params['query_strategy']\n",
    "    )\n",
    "else:\n",
    "    # Si no se encontraron best_params, usar par√°metros por defecto\n",
    "    print(\"Usando par√°metros por defecto\")\n",
    "    test_predictions, test_confidence, test_coverage = improved_similarity_prediction_test(\n",
    "        test_df,\n",
    "        threshold=0.01,\n",
    "        ngram_range=(1, 2),\n",
    "        query_strategy=\"text_acronym\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Predicciones en test completadas:\")\n",
    "print(f\"   - N√∫mero de predicciones: {len(test_predictions)}\")\n",
    "print(f\"   - Confianza promedio: {test_confidence:.4f}\")\n",
    "print(f\"   - Cobertura: {test_coverage:.4f}\")\n",
    "\n",
    "# Crear DataFrame de submission\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'answer': [pred for pred in test_predictions]\n",
    "})\n",
    "\n",
    "# Mostrar distribuci√≥n de las predicciones\n",
    "print(\"\\nDistribuci√≥n de las respuestas predichas:\")\n",
    "print(predictions_df['answer'].value_counts().sort_index())\n",
    "\n",
    "# Guardar el archivo de submission\n",
    "output_filename = 'submission_improved.csv'\n",
    "predictions_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nüéØ Archivo de submission guardado como: {output_filename}\")\n",
    "\n",
    "# Validaci√≥n r√°pida del archivo guardado\n",
    "print(\"\\n=== VALIDACI√ìN DEL ARCHIVO GUARDADO ===\")\n",
    "saved_df = pd.read_csv(output_filename)\n",
    "print(f\"Archivo cargado: {saved_df.shape[0]} filas\")\n",
    "print(\"Primeras 5 filas:\")\n",
    "print(saved_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151524bd",
   "metadata": {},
   "source": [
    "# TD-IDF \n",
    "\n",
    "### **Project Summary: Acronym Expansion System**\n",
    "\n",
    "**Objective:** Build an NLP system to identify the correct long-form expansion of an acronym within a French text.\n",
    "\n",
    "**Methodology & Key Steps:**\n",
    "\n",
    "*   **Data Processing:**\n",
    "    *   Loaded and cleaned JSONL datasets (`train.jsonl`, `test.jsonl`).\n",
    "    *   Handled missing values and removed duplicates.\n",
    "\n",
    "*   **Text Preprocessing:**\n",
    "    *   Unified pipeline for text and acronym options.\n",
    "    *   Applied lowercase conversion, accent removal, and tokenization.\n",
    "    *   Utilized NLTK for stopword removal and spaCy for French lemmatization.\n",
    "\n",
    "*   **Feature Engineering:**\n",
    "    *   Created a TF-IDF vectorizer trained on a combined corpus of all processed texts and options.\n",
    "    *   Incorporated n-grams (unigrams, bigrams, trigrams) to capture contextual phrases.\n",
    "\n",
    "*   **Model & Prediction:**\n",
    "    *   Core model based on **Cosine Similarity** between the TF-IDF vectors of the context (text + acronym) and the potential options.\n",
    "    *   Implemented a confidence threshold to filter weak matches.\n",
    "\n",
    "*   **Validation & Hyperparameter Tuning:**\n",
    "    *   Conducted an extensive grid search to optimize key parameters:\n",
    "        *   Similarity Threshold\n",
    "        *   N-gram Range\n",
    "        *   Query Construction Strategy (e.g., text-only, text+acronym)\n",
    "    *   Systematically evaluated performance to find the best configuration.\n",
    "\n",
    "**Outcome:**\n",
    "*   Successfully generated a `submission_improved.csv` file with predictions for the test set.\n",
    "*   Delivered a robust, explainable model based on semantic similarity rather than a black-box classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b3afb",
   "metadata": {},
   "source": [
    "Result : 0.51538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec0efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET STATISTICS ===\n",
      "Training samples: 492\n",
      "Test samples: 519\n",
      "Validation samples: 148\n",
      "Unique acronyms in train: 77\n",
      "\n",
      "Opciones por pregunta:\n",
      "  M√≠nimo: 2\n",
      "  M√°ximo: 13\n",
      "  Promedio: 4.42\n",
      "\n",
      "=== PREPROCESSING IMPACT ===\n",
      "Longitud promedio texto original: 208.66 chars\n",
      "Longitud promedio texto procesado: 141.72 chars\n",
      "Reducci√≥n: 32.1%\n",
      "\n",
      "=== MODEL PERFORMANCE ===\n",
      "Best validation accuracy: 0.5068\n",
      "Best parameters: {'threshold': 0.005, 'ngram_range': (1, 2), 'query_strategy': 'text_acronym', 'accuracy': 0.5067567567567568}\n"
     ]
    }
   ],
   "source": [
    "# 1. Estad√≠sticas b√°sicas del dataset\n",
    "print(\"=== DATASET STATISTICS ===\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Validation samples: {len(val_final)}\")\n",
    "print(f\"Unique acronyms in train: {train_df['acronym'].nunique()}\")\n",
    "\n",
    "# 2. Distribuci√≥n de opciones por pregunta\n",
    "train_options_count = train_df['options'].apply(len)\n",
    "print(f\"\\nOpciones por pregunta:\")\n",
    "print(f\"  M√≠nimo: {train_options_count.min()}\")\n",
    "print(f\"  M√°ximo: {train_options_count.max()}\")\n",
    "print(f\"  Promedio: {train_options_count.mean():.2f}\")\n",
    "\n",
    "# 3. M√©tricas del preprocesamiento\n",
    "text_lengths_before = train_df['text'].str.len()\n",
    "text_lengths_after = train_df['text_processed'].str.len()\n",
    "print(f\"\\n=== PREPROCESSING IMPACT ===\")\n",
    "print(f\"Longitud promedio texto original: {text_lengths_before.mean():.2f} chars\")\n",
    "print(f\"Longitud promedio texto procesado: {text_lengths_after.mean():.2f} chars\")\n",
    "print(f\"Reducci√≥n: {((text_lengths_before.mean() - text_lengths_after.mean()) / text_lengths_before.mean() * 100):.1f}%\")\n",
    "\n",
    "# 4. Performance del modelo (si tenemos las mejores m√©tricas)\n",
    "if 'best_params' in locals() and 'final_accuracy' in locals():\n",
    "    print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "    print(f\"Best validation accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "# Si no, calculemos la accuracy del validation actual\n",
    "else:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    val_true = val_final['answer_index'].values\n",
    "    val_pred = [p[0] for p in val_predictions]\n",
    "    current_accuracy = accuracy_score(val_true, val_pred)\n",
    "    print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "    print(f\"Current validation accuracy: {current_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
