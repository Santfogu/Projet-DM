{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0414f6d9",
   "metadata": {},
   "source": [
    "# Projet DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf31586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\santi\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\santi\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\santi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/16.3 MB 2.1 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 1.3/16.3 MB 2.9 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 2.1/16.3 MB 3.2 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 3.1/16.3 MB 3.7 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.9/16.3 MB 3.8 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 5.5/16.3 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 7.3/16.3 MB 5.0 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.9/16.3 MB 5.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 11.0/16.3 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 12.8/16.3 MB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.7/16.3 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.3/16.3 MB 6.7 MB/s  0:00:02\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d840f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    \"\"\"Load a .jsonl file. Tries both the given path and the 'data' folder.\n",
    "    Returns a list of dicts.\"\"\"\n",
    "    p = os.path.join('data', filename)\n",
    "    if os.path.exists(p):\n",
    "        data = []\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing a line in {p}: {e}\")\n",
    "        return data\n",
    "    raise FileNotFoundError(f\"File not found: {filename} (tried: {p})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d04bdd7",
   "metadata": {},
   "source": [
    "### 1. Data Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c64e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train records: 492, Test records: 519\n",
      "-- Train head --\n",
      "                                                text acronym  \\\n",
      "0  LRA  limite de rÃ©sistance des attelages PAR po...     PAR   \n",
      "1                              DÃ©signa -tion des PN       PN   \n",
      "2  prÃ©dÃ©terminÃ©es de trains : _x0001_ les masses ...      EM   \n",
      "3  /Commentaires NÂ° AC B81500 thermique:  compati...      AC   \n",
      "4  kilomÃ¨tres/heure (ex : 12 pour 120 km/h), _x00...     TIV   \n",
      "\n",
      "                                             options  \n",
      "0  {'Plan d'action rÃ©gularitÃ©': False, 'Poste d'a...  \n",
      "1  {'Passages Ã  niveau : fichier des pn, recensem...  \n",
      "2  {'EMERAINVILLE PONTAULT COMBAULT': False, 'Eng...  \n",
      "3  {'ACcÃ¨s': False, 'Agent d'aCcompagnement ': Fa...  \n",
      "4  {'THIVIERS': False, 'Trafic international voya...  \n",
      "\n",
      "-- Info --\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 492 entries, 0 to 491\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     492 non-null    object\n",
      " 1   acronym  492 non-null    object\n",
      " 2   options  492 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the data \n",
    "train_path = 'train_v2.jsonl'\n",
    "test_path = 'test_v4.jsonl'\n",
    "\n",
    "train_data = load_jsonl(train_path)\n",
    "test_data = load_jsonl(test_path)\n",
    "\n",
    "print(f\"Train records: {len(train_data)}, Test records: {len(test_data)}\")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Quick preview\n",
    "print(\"-- Train head --\")\n",
    "print(train_df.head())\n",
    "print(\"\\n-- Info --\")\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccd4a2",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "784b460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['text', 'acronym', 'options']\n",
      "Missing values per column:\n",
      " text       0\n",
      "acronym    0\n",
      "options    0\n",
      "dtype: int64\n",
      "After cleaning, shape: (492, 3)\n"
     ]
    }
   ],
   "source": [
    "# Basic cleaning: columns, missing values, duplicates\n",
    "print(\"Columns:\", list(train_df.columns))\n",
    "print(\"Missing values per column:\\n\", train_df.isna().sum())\n",
    "\n",
    "train_df = train_df.drop_duplicates(subset=['text', 'acronym']).reset_index(drop=True)\n",
    "\n",
    "for col in train_df.select_dtypes(include='object').columns:\n",
    "    train_df[col] = train_df[col].fillna('')\n",
    "\n",
    "print(\"After cleaning, shape:\", train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac1be1",
   "metadata": {},
   "source": [
    "### 3. Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b06ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train texts...\n",
      "Processing test texts...\n",
      "Processing test texts...\n",
      "Processing train options...\n",
      "Processing train options...\n",
      "Processing test options...\n",
      "Processing test options...\n",
      "âœ… Preprocessing completed\n",
      "Sample processed text: lra limite resistance attelage poste regulation pl pleine lign pn passage avoir niveau rfn reseau fe...\n",
      "Sample processed options keys: ['plan regularite', 'poste et de regulation assurer le commande de installation de signalisation et le gestion de le circulation de huit ligne avoir grand vitesse', 'pont de', 'plan regional']\n",
      "âœ… Preprocessing completed\n",
      "Sample processed text: lra limite resistance attelage poste regulation pl pleine lign pn passage avoir niveau rfn reseau fe...\n",
      "Sample processed options keys: ['plan regularite', 'poste et de regulation assurer le commande de installation de signalisation et le gestion de le circulation de huit ligne avoir grand vitesse', 'pont de', 'plan regional']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources (once)\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Load spaCy French model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "def preprocessing(text, options=False):\n",
    "    \"\"\"Unified preprocessing for text and options.\"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = unidecode(text)\n",
    "    try:\n",
    "        tokens = word_tokenize(text, language='french')\n",
    "    except:\n",
    "        tokens = text.split()\n",
    "    if options:\n",
    "        filtered_tokens = [w for w in tokens if w.isalpha()]\n",
    "    else:\n",
    "        filtered_tokens = [w for w in tokens if w not in stop_words and w.isalpha()]\n",
    "    text_to_lemmatize = \" \".join(filtered_tokens)\n",
    "    try:\n",
    "        doc = nlp(text_to_lemmatize)\n",
    "        lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
    "        return \" \".join(lemmas)\n",
    "    except Exception as e:\n",
    "        print(f\"Lemmatization error: {e}\")\n",
    "        return text_to_lemmatize\n",
    "\n",
    "print(\"Processing train texts...\")\n",
    "train_df['text_processed'] = train_df['text'].apply(preprocessing)\n",
    "print(\"Processing test texts...\")\n",
    "test_df['text_processed'] = test_df['text'].apply(preprocessing)\n",
    "\n",
    "def process_options_dict(options_dict):\n",
    "    \"\"\"Process training options (dict).\"\"\"\n",
    "    return {preprocessing(k, True): v for k, v in options_dict.items()}\n",
    "\n",
    "def process_options_list(options_list):\n",
    "    \"\"\"Process test options (list).\"\"\"\n",
    "    return [preprocessing(opt, True) for opt in options_list]\n",
    "\n",
    "print(\"Processing train options...\")\n",
    "train_df['options_processed'] = train_df['options'].apply(process_options_dict)\n",
    "print(\"Processing test options...\")\n",
    "test_df['options_processed'] = test_df['options'].apply(process_options_list)\n",
    "\n",
    "print(\"âœ… Preprocessing completed\")\n",
    "print(f\"Sample processed text: {train_df['text_processed'].iloc[0][:100]}...\")\n",
    "print(f\"Sample processed options keys: {list(train_df['options_processed'].iloc[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec0850",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049daaf1",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a6f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_final, val_final = train_test_split(train_df, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a39c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF vocabulary...\n",
      "âœ… Global TF-IDF trained with 7425 terms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Building TF-IDF vocabulary...\")\n",
    "all_texts_for_tfidf = []\n",
    "all_texts_for_tfidf.extend(train_final['text_processed'].tolist())\n",
    "for options_dict in train_final['options_processed']:\n",
    "    all_texts_for_tfidf.extend(options_dict.keys())\n",
    "all_texts_for_tfidf.extend(test_df['text_processed'].tolist())\n",
    "\n",
    "global_tfidf = TfidfVectorizer(ngram_range=(1,3), min_df=2, max_features=10000)\n",
    "global_tfidf.fit(all_texts_for_tfidf)\n",
    "print(f\"âœ… Global TF-IDF trained with {len(global_tfidf.vocabulary_)} terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbbb845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>acronym</th>\n",
       "      <th>options</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>options_processed</th>\n",
       "      <th>answer_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>VENISSIEUX Ã  AMBERIEU  Article B101 Domaine de...</td>\n",
       "      <td>EF</td>\n",
       "      <td>{'Entreprise Ferroviaire :   Toute entreprise ...</td>\n",
       "      <td>venissieux avoir amberieu article domaine circ...</td>\n",
       "      <td>{'entreprise ferroviaire tout entreprise avoir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Boingneville PLBV â€“ Poste 1MALESHERBES(1) â€“ Et...</td>\n",
       "      <td>BV</td>\n",
       "      <td>{'Bassin Versant': False, 'BÃ¢timent des Voyage...</td>\n",
       "      <td>boingnevill plbv poste etablissement pl suscep...</td>\n",
       "      <td>{'bassin verser': False, 'batiment de voyageur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>FeuquiÃ¨res Fressenneville - PL 30Woincourt - P...</td>\n",
       "      <td>PL</td>\n",
       "      <td>{'Panneaux lumineux': False, 'Pleine Ligne.   ...</td>\n",
       "      <td>feuquiere fressennevill pl pl kilomÃ¨tre ligne ...</td>\n",
       "      <td>{'panneau lumineux': False, 'pleine lign etabl...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>franchissement des signaux d'arrÃªt Ligne Ã©quip...</td>\n",
       "      <td>DAAT</td>\n",
       "      <td>{'Diagnostique Amiante Avant Travaux': False, ...</td>\n",
       "      <td>franchissemer signal ligne equipe dispositif a...</td>\n",
       "      <td>{'diagnostique amiant avant travail': False, '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Page 4 NPDC-RT-2211B- Version 01 du 15-12-2014...</td>\n",
       "      <td>EF</td>\n",
       "      <td>{'Equipement fixe': False, 'Essai de Frein': F...</td>\n",
       "      <td>page version article domaine circulation appar...</td>\n",
       "      <td>{'equipement fixe': False, 'essai de frein': F...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text acronym  \\\n",
       "184  VENISSIEUX Ã  AMBERIEU  Article B101 Domaine de...      EF   \n",
       "252  Boingneville PLBV â€“ Poste 1MALESHERBES(1) â€“ Et...      BV   \n",
       "397  FeuquiÃ¨res Fressenneville - PL 30Woincourt - P...      PL   \n",
       "325  franchissement des signaux d'arrÃªt Ligne Ã©quip...    DAAT   \n",
       "136  Page 4 NPDC-RT-2211B- Version 01 du 15-12-2014...      EF   \n",
       "\n",
       "                                               options  \\\n",
       "184  {'Entreprise Ferroviaire :   Toute entreprise ...   \n",
       "252  {'Bassin Versant': False, 'BÃ¢timent des Voyage...   \n",
       "397  {'Panneaux lumineux': False, 'Pleine Ligne.   ...   \n",
       "325  {'Diagnostique Amiante Avant Travaux': False, ...   \n",
       "136  {'Equipement fixe': False, 'Essai de Frein': F...   \n",
       "\n",
       "                                        text_processed  \\\n",
       "184  venissieux avoir amberieu article domaine circ...   \n",
       "252  boingnevill plbv poste etablissement pl suscep...   \n",
       "397  feuquiere fressennevill pl pl kilomÃ¨tre ligne ...   \n",
       "325  franchissemer signal ligne equipe dispositif a...   \n",
       "136  page version article domaine circulation appar...   \n",
       "\n",
       "                                     options_processed  answer_index  \n",
       "184  {'entreprise ferroviaire tout entreprise avoir...             0  \n",
       "252  {'bassin verser': False, 'batiment de voyageur...             1  \n",
       "397  {'panneau lumineux': False, 'pleine lign etabl...            -1  \n",
       "325  {'diagnostique amiant avant travail': False, '...             1  \n",
       "136  {'equipement fixe': False, 'essai de frein': F...             2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_answer_index(row):\n",
    "    options = row['options_processed']\n",
    "    for idx, option in enumerate(options):\n",
    "        if options[option] == True:\n",
    "            return idx\n",
    "    return -1\n",
    "val_final['answer_index'] = val_final.apply(get_answer_index, axis=1)\n",
    "val_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1ae1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL VALIDATION WITH MULTIPLE PARAMETERS ===\n",
      "\n",
      "  Threshold 0.000: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.000: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.001: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.001: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.005: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.005: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.010: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Threshold 0.010: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Threshold 0.020: Accuracy=0.4595, Avg_conf=0.0606, Coverage=0.6554\n",
      "  Threshold 0.020: Accuracy=0.4595, Avg_conf=0.0606, Coverage=0.6554\n",
      "  Threshold 0.050: Accuracy=0.3581, Avg_conf=0.0606, Coverage=0.4392\n",
      "  Threshold 0.050: Accuracy=0.3581, Avg_conf=0.0606, Coverage=0.4392\n",
      "  Threshold 0.100: Accuracy=0.2973, Avg_conf=0.0606, Coverage=0.1959\n",
      "  Threshold 0.100: Accuracy=0.2973, Avg_conf=0.0606, Coverage=0.1959\n",
      "  Threshold 0.150: Accuracy=0.2905, Avg_conf=0.0606, Coverage=0.1149\n",
      "  Threshold 0.150: Accuracy=0.2905, Avg_conf=0.0606, Coverage=0.1149\n",
      "  Threshold 0.200: Accuracy=0.2635, Avg_conf=0.0606, Coverage=0.0676\n",
      "  Threshold 0.200: Accuracy=0.2635, Avg_conf=0.0606, Coverage=0.0676\n",
      "  Threshold 0.250: Accuracy=0.2568, Avg_conf=0.0606, Coverage=0.0473\n",
      "  Threshold 0.250: Accuracy=0.2568, Avg_conf=0.0606, Coverage=0.0473\n",
      "  ngram_range (1, 1): Accuracy=0.5068, Avg_conf=0.0903, Coverage=0.7500\n",
      "  ngram_range (1, 1): Accuracy=0.5068, Avg_conf=0.0903, Coverage=0.7500\n",
      "  ngram_range (1, 2): Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  ngram_range (1, 2): Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  ngram_range (1, 3): Accuracy=0.4797, Avg_conf=0.0468, Coverage=0.7095\n",
      "  ngram_range (1, 3): Accuracy=0.4797, Avg_conf=0.0468, Coverage=0.7095\n",
      "  ngram_range (2, 2): Accuracy=0.3851, Avg_conf=0.0192, Coverage=0.2432\n",
      "  ngram_range (2, 2): Accuracy=0.3851, Avg_conf=0.0192, Coverage=0.2432\n",
      "  ngram_range (2, 3): Accuracy=0.3851, Avg_conf=0.0127, Coverage=0.2365\n",
      "  ngram_range (2, 3): Accuracy=0.3851, Avg_conf=0.0127, Coverage=0.2365\n",
      "  ngram_range (3, 3): Accuracy=0.2635, Avg_conf=0.0040, Coverage=0.0270\n",
      "  ngram_range (3, 3): Accuracy=0.2635, Avg_conf=0.0040, Coverage=0.0270\n",
      "  Query 'text_only': Accuracy=0.5135, Avg_conf=0.0568, Coverage=0.7027\n",
      "  Query 'text_only': Accuracy=0.5135, Avg_conf=0.0568, Coverage=0.7027\n",
      "  Query 'acronym_only': Accuracy=0.2905, Avg_conf=0.0724, Coverage=0.2432\n",
      "  Query 'acronym_only': Accuracy=0.2905, Avg_conf=0.0724, Coverage=0.2432\n",
      "  Query 'text_acronym': Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Query 'text_acronym': Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Query 'acronym_text': Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Query 'acronym_text': Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Query 'weighted_text': Accuracy=0.4932, Avg_conf=0.0584, Coverage=0.7297\n",
      "  Query 'weighted_text': Accuracy=0.4932, Avg_conf=0.0584, Coverage=0.7297\n",
      "  Fallback 'first': Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Fallback 'first': Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Fallback 'random': Accuracy=0.5473, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Fallback 'random': Accuracy=0.5473, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Fallback 'last': Accuracy=0.5135, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Fallback 'last': Accuracy=0.5135, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Fallback 'middle': Accuracy=0.5203, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Fallback 'middle': Accuracy=0.5203, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Combo 1: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "    Params: {'threshold': 0.01, 'ngram_range': (1, 2), 'query_strategy': 'text_acronym'}\n",
      "  Combo 1: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "    Params: {'threshold': 0.01, 'ngram_range': (1, 2), 'query_strategy': 'text_acronym'}\n",
      "  Combo 2: Accuracy=0.3108, Avg_conf=0.0447, Coverage=0.2703\n",
      "    Params: {'threshold': 0.05, 'ngram_range': (1, 3), 'query_strategy': 'weighted_text'}\n",
      "  Combo 2: Accuracy=0.3108, Avg_conf=0.0447, Coverage=0.2703\n",
      "    Params: {'threshold': 0.05, 'ngram_range': (1, 3), 'query_strategy': 'weighted_text'}\n",
      "  Combo 3: Accuracy=0.4865, Avg_conf=0.0568, Coverage=0.6351\n",
      "    Params: {'threshold': 0.02, 'ngram_range': (1, 2), 'query_strategy': 'text_only'}\n",
      "  Combo 3: Accuracy=0.4865, Avg_conf=0.0568, Coverage=0.6351\n",
      "    Params: {'threshold': 0.02, 'ngram_range': (1, 2), 'query_strategy': 'text_only'}\n",
      "  Combo 4: Accuracy=0.2635, Avg_conf=0.0127, Coverage=0.0405\n",
      "    Params: {'threshold': 0.1, 'ngram_range': (2, 3), 'query_strategy': 'acronym_text'}\n",
      "  Combo 4: Accuracy=0.2635, Avg_conf=0.0127, Coverage=0.0405\n",
      "    Params: {'threshold': 0.1, 'ngram_range': (2, 3), 'query_strategy': 'acronym_text'}\n",
      "  Threshold 0.005: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.005: Accuracy=0.5068, Avg_conf=0.0606, Coverage=0.7500\n",
      "  Threshold 0.008: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7432\n",
      "  Threshold 0.008: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7432\n",
      "  Threshold 0.010: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Threshold 0.010: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Threshold 0.012: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Threshold 0.012: Accuracy=0.5000, Avg_conf=0.0606, Coverage=0.7230\n",
      "  Threshold 0.015: Accuracy=0.4730, Avg_conf=0.0606, Coverage=0.6824\n",
      "  Threshold 0.015: Accuracy=0.4730, Avg_conf=0.0606, Coverage=0.6824\n",
      "  Threshold 0.018: Accuracy=0.4730, Avg_conf=0.0606, Coverage=0.6689\n",
      "  Threshold 0.018: Accuracy=0.4730, Avg_conf=0.0606, Coverage=0.6689\n",
      "  Threshold 0.020: Accuracy=0.4595, Avg_conf=0.0606, Coverage=0.6554\n",
      "  Threshold 0.020: Accuracy=0.4595, Avg_conf=0.0606, Coverage=0.6554\n",
      "  Threshold 0.025: Accuracy=0.4392, Avg_conf=0.0606, Coverage=0.6014\n",
      "\n",
      "ðŸŽ¯ BEST THRESHOLD: 0.005 with accuracy: 0.5068\n",
      "\n",
      "7. RANDOM PARAM SEARCH:\n",
      "  Threshold 0.025: Accuracy=0.4392, Avg_conf=0.0606, Coverage=0.6014\n",
      "\n",
      "ðŸŽ¯ BEST THRESHOLD: 0.005 with accuracy: 0.5068\n",
      "\n",
      "7. RANDOM PARAM SEARCH:\n",
      "ðŸ”¥ New best: Accuracy=0.2635, threshold=0.01, ngram_range=(2, 2), query_strategy='acronym_only'\n",
      "ðŸ”¥ New best: Accuracy=0.2635, threshold=0.01, ngram_range=(2, 2), query_strategy='acronym_only'\n",
      "ðŸ”¥ New best: Accuracy=0.3176, threshold=0.02, ngram_range=(2, 3), query_strategy='text_acronym'\n",
      "ðŸ”¥ New best: Accuracy=0.3176, threshold=0.02, ngram_range=(2, 3), query_strategy='text_acronym'\n",
      "ðŸ”¥ New best: Accuracy=0.3851, threshold=0.01, ngram_range=(2, 3), query_strategy='text_only'\n",
      "ðŸ”¥ New best: Accuracy=0.3851, threshold=0.01, ngram_range=(2, 3), query_strategy='text_only'\n",
      "ðŸ”¥ New best: Accuracy=0.4932, threshold=0.0, ngram_range=(1, 3), query_strategy='weighted_text'\n",
      "ðŸ”¥ New best: Accuracy=0.4932, threshold=0.0, ngram_range=(1, 3), query_strategy='weighted_text'\n",
      "ðŸ”¥ New best: Accuracy=0.5000, threshold=0.01, ngram_range=(1, 2), query_strategy='text_acronym'\n",
      "ðŸ”¥ New best: Accuracy=0.5000, threshold=0.01, ngram_range=(1, 2), query_strategy='text_acronym'\n",
      "ðŸ”¥ New best: Accuracy=0.5068, threshold=0.005, ngram_range=(1, 2), query_strategy='text_acronym'\n",
      "ðŸ”¥ New best: Accuracy=0.5068, threshold=0.005, ngram_range=(1, 2), query_strategy='text_acronym'\n",
      "ðŸ”¥ New best: Accuracy=0.5270, threshold=0.001, ngram_range=(1, 2), query_strategy='text_only'\n",
      "ðŸ”¥ New best: Accuracy=0.5270, threshold=0.001, ngram_range=(1, 2), query_strategy='text_only'\n",
      "\n",
      "ðŸ† BEST PARAMETERS FOUND:\n",
      "  threshold: 0.001\n",
      "  ngram_range: (1, 2)\n",
      "  query_strategy: text_only\n",
      "  accuracy: 0.527027027027027\n",
      "\n",
      "8. FINAL EVAL WITH BEST PARAMETERS:\n",
      "\n",
      "ðŸ† BEST PARAMETERS FOUND:\n",
      "  threshold: 0.001\n",
      "  ngram_range: (1, 2)\n",
      "  query_strategy: text_only\n",
      "  accuracy: 0.527027027027027\n",
      "\n",
      "8. FINAL EVAL WITH BEST PARAMETERS:\n",
      "âœ… Final result: Accuracy=0.5270\n",
      "âœ… Final result: Accuracy=0.5270\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def improved_similarity_prediction_validation(df, threshold=0.01, ngram_range=(1, 2),\n",
    "                                            use_acronym=True, query_strategy=\"text_acronym\",\n",
    "                                            fallback_strategy=\"first\"):\n",
    "    \"\"\"Improved similarity-based prediction for validation.\"\"\"\n",
    "    predictions = []\n",
    "    validation_scores = []\n",
    "    confidence_scores = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            text = row['text_processed']\n",
    "            acronym = row['acronym']\n",
    "            options = row['options_processed']\n",
    "\n",
    "            if query_strategy == \"text_only\":\n",
    "                query = text\n",
    "            elif query_strategy == \"acronym_only\":\n",
    "                query = acronym\n",
    "            elif query_strategy == \"text_acronym\":\n",
    "                query = f\"{text} {acronym}\"\n",
    "            elif query_strategy == \"acronym_text\":\n",
    "                query = f\"{acronym} {text}\"\n",
    "            elif query_strategy == \"weighted_text\":\n",
    "                query = f\"{text} {text} {acronym}\"\n",
    "            else:\n",
    "                query = f\"{text} {acronym}\"\n",
    "\n",
    "            vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "            options_list = list(options.keys())\n",
    "            corpus = [query] + options_list\n",
    "            tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_score = similarities[best_idx]\n",
    "\n",
    "            confidence_scores.append(best_score)\n",
    "\n",
    "            if best_score > threshold:\n",
    "                validation_scores.append(1 if best_idx == row['answer_index'] else 0)\n",
    "                predictions.append([best_idx])\n",
    "            else:\n",
    "                if fallback_strategy == \"first\":\n",
    "                    fallback_idx = 0\n",
    "                elif fallback_strategy == \"random\":\n",
    "                    fallback_idx = np.random.randint(0, len(options_list))\n",
    "                elif fallback_strategy == \"last\":\n",
    "                    fallback_idx = len(options_list) - 1\n",
    "                elif fallback_strategy == \"middle\":\n",
    "                    fallback_idx = len(options_list) // 2\n",
    "                else:\n",
    "                    fallback_idx = 0\n",
    "                predictions.append([fallback_idx])\n",
    "                validation_scores.append(1 if fallback_idx == row['answer_index'] else 0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Row {idx} error: {e}\")\n",
    "            predictions.append([0])\n",
    "            validation_scores.append(0)\n",
    "\n",
    "    accuracy = np.mean(validation_scores) if validation_scores else 0\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    coverage = np.mean([1 if score > threshold else 0 for score in confidence_scores]) if confidence_scores else 0\n",
    "\n",
    "    return predictions, accuracy, avg_confidence, coverage\n",
    "\n",
    "print(\"=== FULL VALIDATION WITH MULTIPLE PARAMETERS ===\\n\")\n",
    "\n",
    "thresholds = [0, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "for thresh in thresholds:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(val_final, threshold=thresh)\n",
    "    print(f\"  Threshold {thresh:.3f}: Accuracy={accuracy:.4f}, Avg_conf={avg_conf:.4f}, Coverage={coverage:.4f}\")\n",
    "\n",
    "ngram_ranges = [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n",
    "for ngram in ngram_ranges:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(val_final, ngram_range=ngram)\n",
    "    print(f\"  ngram_range {ngram}: Accuracy={accuracy:.4f}, Avg_conf={avg_conf:.4f}, Coverage={coverage:.4f}\")\n",
    "\n",
    "query_strategies = [\"text_only\", \"acronym_only\", \"text_acronym\", \"acronym_text\", \"weighted_text\"]\n",
    "for strategy in query_strategies:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(val_final, query_strategy=strategy)\n",
    "    print(f\"  Query '{strategy}': Accuracy={accuracy:.4f}, Avg_conf={avg_conf:.4f}, Coverage={coverage:.4f}\")\n",
    "\n",
    "fallback_strategies = [\"first\", \"random\", \"last\", \"middle\"]\n",
    "for fallback in fallback_strategies:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(val_final, fallback_strategy=fallback)\n",
    "    print(f\"  Fallback '{fallback}': Accuracy={accuracy:.4f}, Avg_conf={avg_conf:.4f}, Coverage={coverage:.4f}\")\n",
    "\n",
    "combinations = [\n",
    "    {\"threshold\": 0.01, \"ngram_range\": (1, 2), \"query_strategy\": \"text_acronym\"},\n",
    "    {\"threshold\": 0.05, \"ngram_range\": (1, 3), \"query_strategy\": \"weighted_text\"},\n",
    "    {\"threshold\": 0.02, \"ngram_range\": (1, 2), \"query_strategy\": \"text_only\"},\n",
    "    {\"threshold\": 0.1, \"ngram_range\": (2, 3), \"query_strategy\": \"acronym_text\"},\n",
    "]\n",
    "for i, combo in enumerate(combinations):\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(val_final, **combo)\n",
    "    print(f\"  Combo {i+1}: Accuracy={accuracy:.4f}, Avg_conf={avg_conf:.4f}, Coverage={coverage:.4f}\")\n",
    "    print(f\"    Params: {combo}\")\n",
    "\n",
    "fine_thresholds = [0.005, 0.008, 0.01, 0.012, 0.015, 0.018, 0.02, 0.025]\n",
    "best_threshold = 0\n",
    "best_accuracy = 0\n",
    "for thresh in fine_thresholds:\n",
    "    _, accuracy, avg_conf, coverage = improved_similarity_prediction_validation(val_final, threshold=thresh)\n",
    "    print(f\"  Threshold {thresh:.3f}: Accuracy={accuracy:.4f}, Avg_conf={avg_conf:.4f}, Coverage={coverage:.4f}\")\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = thresh\n",
    "print(f\"\\nðŸŽ¯ BEST THRESHOLD: {best_threshold} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "def find_best_parameters(validation_df, n_trials=50):\n",
    "    \"\"\"Random search for best params.\"\"\"\n",
    "    best_params = {}\n",
    "    best_accuracy = 0\n",
    "    thresholds = [0, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "    ngram_ranges = [(1,1),(1,2),(1,3),(2,2),(2,3)]\n",
    "    query_strategies = [\"text_only\",\"acronym_only\",\"text_acronym\",\"weighted_text\"]\n",
    "    for i in range(n_trials):\n",
    "        threshold = np.random.choice(thresholds)\n",
    "        ngram_range = ngram_ranges[np.random.randint(len(ngram_ranges))]\n",
    "        query_strategy = query_strategies[np.random.randint(len(query_strategies))]\n",
    "        try:\n",
    "            _, accuracy, _, _ = improved_similarity_prediction_validation(validation_df, threshold=threshold, ngram_range=ngram_range, query_strategy=query_strategy)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'threshold': threshold, 'ngram_range': ngram_range, 'query_strategy': query_strategy, 'accuracy': accuracy}\n",
    "                print(f\"ðŸ”¥ New best: Accuracy={accuracy:.4f}, threshold={threshold}, ngram_range={ngram_range}, query_strategy='{query_strategy}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Trial {i} error: {e}\")\n",
    "            continue\n",
    "    return best_params\n",
    "\n",
    "print(\"\\n7. RANDOM PARAM SEARCH:\")\n",
    "best_params = find_best_parameters(val_final, n_trials=30)\n",
    "print(\"\\nðŸ† BEST PARAMETERS FOUND:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if best_params:\n",
    "    print(\"\\n8. FINAL EVAL WITH BEST PARAMETERS:\")\n",
    "    final_predictions, final_accuracy, final_confidence, final_coverage = improved_similarity_prediction_validation(val_final, threshold=best_params['threshold'], ngram_range=best_params['ngram_range'], query_strategy=best_params['query_strategy'])\n",
    "    print(f\"âœ… Final result: Accuracy={final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c0afbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MAKING FINAL TEST PREDICTIONS ===\n",
      "Using best params: {'threshold': 0.001, 'ngram_range': (1, 2), 'query_strategy': 'text_only', 'accuracy': 0.527027027027027}\n",
      "âœ… Test predictions done: 519 items\n",
      "   - Avg confidence: 0.0449\n",
      "   - Coverage: 0.6686\n",
      "\n",
      "Predicted answer distribution:\n",
      "answer\n",
      "[0]     258\n",
      "[1]     119\n",
      "[2]      63\n",
      "[3]      62\n",
      "[4]       5\n",
      "[5]       2\n",
      "[6]       3\n",
      "[7]       3\n",
      "[8]       1\n",
      "[9]       2\n",
      "[12]      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved submission: submission_improved.csv\n",
      "\n",
      "=== CHECK SAVED FILE ===\n",
      "Loaded file rows: 519\n",
      "First 5 rows:\n",
      "   id answer\n",
      "0   0    [1]\n",
      "1   1    [1]\n",
      "2   2    [0]\n",
      "3   3    [1]\n",
      "4   4    [0]\n",
      "âœ… Test predictions done: 519 items\n",
      "   - Avg confidence: 0.0449\n",
      "   - Coverage: 0.6686\n",
      "\n",
      "Predicted answer distribution:\n",
      "answer\n",
      "[0]     258\n",
      "[1]     119\n",
      "[2]      63\n",
      "[3]      62\n",
      "[4]       5\n",
      "[5]       2\n",
      "[6]       3\n",
      "[7]       3\n",
      "[8]       1\n",
      "[9]       2\n",
      "[12]      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved submission: submission_improved.csv\n",
      "\n",
      "=== CHECK SAVED FILE ===\n",
      "Loaded file rows: 519\n",
      "First 5 rows:\n",
      "   id answer\n",
      "0   0    [1]\n",
      "1   1    [1]\n",
      "2   2    [0]\n",
      "3   3    [1]\n",
      "4   4    [0]\n"
     ]
    }
   ],
   "source": [
    "### 6. Final test prediction\n",
    "\n",
    "def improved_similarity_prediction_test(df, threshold=0.01, ngram_range=(1, 2),\n",
    "                                      query_strategy=\"text_acronym\", fallback_strategy=\"first\"):\n",
    "    \"\"\"Produce predictions for the test set.\"\"\"\n",
    "    predictions = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            text = row['text_processed']\n",
    "            acronym = row['acronym']\n",
    "            options = row['options_processed']  # test options list\n",
    "            \n",
    "            if query_strategy == \"text_only\":\n",
    "                query = text\n",
    "            elif query_strategy == \"acronym_only\":\n",
    "                query = acronym\n",
    "            elif query_strategy == \"text_acronym\":\n",
    "                query = f\"{text} {acronym}\"\n",
    "            elif query_strategy == \"acronym_text\":\n",
    "                query = f\"{acronym} {text}\"\n",
    "            elif query_strategy == \"weighted_text\":\n",
    "                query = f\"{text} {text} {acronym}\"\n",
    "            else:\n",
    "                query = f\"{text} {acronym}\"\n",
    "\n",
    "            vectorizer = TfidfVectorizer(ngram_range=ngram_range)\n",
    "            options_list = options\n",
    "            corpus = [query] + options_list\n",
    "            tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_score = similarities[best_idx]\n",
    "\n",
    "            confidence_scores.append(best_score)\n",
    "\n",
    "            if best_score > threshold:\n",
    "                predictions.append([best_idx])\n",
    "            else:\n",
    "                if fallback_strategy == \"first\":\n",
    "                    fallback_idx = 0\n",
    "                elif fallback_strategy == \"random\":\n",
    "                    fallback_idx = np.random.randint(0, len(options_list))\n",
    "                elif fallback_strategy == \"last\":\n",
    "                    fallback_idx = len(options_list) - 1\n",
    "                elif fallback_strategy == \"middle\":\n",
    "                    fallback_idx = len(options_list) // 2\n",
    "                else:\n",
    "                    fallback_idx = 0\n",
    "                predictions.append([fallback_idx])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Row {idx} error: {e}\")\n",
    "            predictions.append([0])\n",
    "            confidence_scores.append(0)\n",
    "    \n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    coverage = np.mean([1 if score > threshold else 0 for score in confidence_scores]) if confidence_scores else 0\n",
    "    \n",
    "    return predictions, avg_confidence, coverage\n",
    "\n",
    "print(\"=== MAKING FINAL TEST PREDICTIONS ===\")\n",
    "\n",
    "if 'best_params' in locals() and best_params:\n",
    "    print(f\"Using best params: {best_params}\")\n",
    "    test_predictions, test_confidence, test_coverage = improved_similarity_prediction_test(\n",
    "        test_df,\n",
    "        threshold=best_params['threshold'],\n",
    "        ngram_range=best_params['ngram_range'],\n",
    "        query_strategy=best_params['query_strategy']\n",
    "    )\n",
    "else:\n",
    "    print(\"Using default params\")\n",
    "    test_predictions, test_confidence, test_coverage = improved_similarity_prediction_test(\n",
    "        test_df,\n",
    "        threshold=0.01,\n",
    "        ngram_range=(1, 2),\n",
    "        query_strategy=\"text_acronym\"\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Test predictions done: {len(test_predictions)} items\")\n",
    "print(f\"   - Avg confidence: {test_confidence:.4f}\")\n",
    "print(f\"   - Coverage: {test_coverage:.4f}\")\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'answer': [pred for pred in test_predictions]\n",
    "})\n",
    "\n",
    "print(\"\\nPredicted answer distribution:\")\n",
    "print(predictions_df['answer'].value_counts().sort_index())\n",
    "\n",
    "output_filename = 'submission_improved.csv'\n",
    "predictions_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nSaved submission: {output_filename}\")\n",
    "\n",
    "print(\"\\n=== CHECK SAVED FILE ===\")\n",
    "saved_df = pd.read_csv(output_filename)\n",
    "print(f\"Loaded file rows: {saved_df.shape[0]}\")\n",
    "print(\"First 5 rows:\")\n",
    "print(saved_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
