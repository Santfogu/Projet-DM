{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e195996",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m f1_score, precision_score, recall_score\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# === 1. Get predictions on the validation set ===\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(val_dataset)\n\u001b[0;32m      7\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mlabel_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# === 1. Get predictions on the validation set ===\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Convert validation dataset back to pandas for grouping\n",
    "val_df = pd.DataFrame(val_dataset)\n",
    "\n",
    "# === 2. Attach predicted labels to each example ===\n",
    "val_df[\"pred_label\"] = pred_labels\n",
    "val_df[\"true_label\"] = true_labels\n",
    "\n",
    "# === 3. Compute per-acronym F1 based on set comparison ===\n",
    "results = []\n",
    "for acronym, group in val_df.groupby(\"acronym\"):\n",
    "    # Get sets of options judged true\n",
    "    predicted_true = set(group.loc[group[\"pred_label\"] == 1, \"option_text\"])\n",
    "    actual_true = set(group.loc[group[\"true_label\"] == 1, \"option_text\"])\n",
    "    \n",
    "    VP = len(predicted_true & actual_true)\n",
    "    FP = len(predicted_true - actual_true)\n",
    "    FN = len(actual_true - predicted_true)\n",
    "    \n",
    "    precision = VP / (VP + FP) if (VP + FP) > 0 else 0\n",
    "    recall = VP / (VP + FN) if (VP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        \"acronym\": acronym,\n",
    "        \"VP\": VP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "\n",
    "# === 4. Display per-acronym and global F1 ===\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "# Weighted/global averages\n",
    "global_VP = results_df[\"VP\"].sum()\n",
    "global_FP = results_df[\"FP\"].sum()\n",
    "global_FN = results_df[\"FN\"].sum()\n",
    "\n",
    "global_precision = global_VP / (global_VP + global_FP) if (global_VP + global_FP) > 0 else 0\n",
    "global_recall = global_VP / (global_VP + global_FN) if (global_VP + global_FN) > 0 else 0\n",
    "global_f1 = 2 * global_precision * global_recall / (global_precision + global_recall) if (global_precision + global_recall) > 0 else 0\n",
    "\n",
    "print(\"\\nðŸ“Š Global Metrics (based on set-level comparison):\")\n",
    "print(f\"Precision: {global_precision:.3f}\")\n",
    "print(f\"Recall:    {global_recall:.3f}\")\n",
    "print(f\"F1-score:  {global_f1:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
